{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import csv \n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row(rows, filename):\n",
    "    with open(filename, 'a', encoding='utf_8_sig') as toWrite:\n",
    "        writer = csv.writer(toWrite, lineterminator='\\n')\n",
    "        writer.writerow(rows)\n",
    "    toWrite.close()\n",
    "        \n",
    "def get_soup(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
    "\n",
    "    # fetching the url, raising error if operation fails\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "\n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ads_urls(url):\n",
    "   \n",
    "    soup = get_soup(url)\n",
    "\n",
    "    cities_links = [];\n",
    "    \n",
    "    for title in soup.find_all('h3'):\n",
    "        data = {\n",
    "           'city': '',\n",
    "           'url': '',        \n",
    "        }\n",
    "        try:\n",
    "            data['url'] = title.find('a').get('href')\n",
    "            data['city'] = title.get_text(strip=True, separator=',').strip().split(',')[0]\n",
    "            cities_links.append(data);\n",
    "        except:\n",
    "            print('Failed to get url '+title.text)\n",
    "        \n",
    "\n",
    "            \n",
    "    return cities_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(city_url):\n",
    "    \n",
    "    \n",
    "    url = city_url['url']\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # logic to parse and return data about apartment\n",
    "    try:\n",
    "        price = re.sub('[^0-9\\,]', '', soup.find(\"span\", \"price-eur\").text)\n",
    "        city = city_url['city']\n",
    "        years = ''\n",
    "        area = ''\n",
    "        heat = ''\n",
    "        rooms = ''\n",
    "        floor = ''\n",
    "        heating_type = ''\n",
    "        building_type = ''\n",
    "        installation = ''\n",
    "#         created_at = ''\n",
    "#         updated_at = ''\n",
    "\n",
    "        simple_stats = soup.find(\"div\", class_=\"obj-stats simple\")\n",
    "        simple_stats_features = []\n",
    "\n",
    "        for stat in simple_stats.find_all(\"dt\"):\n",
    "            feature_key = stat.text.strip()\n",
    "            simple_stats_features.append([feature_key])\n",
    "\n",
    "        for key, stat in enumerate(simple_stats.find_all(\"dd\")):\n",
    "            feature_key = stat.text.strip()\n",
    "            simple_stats_features[key].append(feature_key)\n",
    "\n",
    "#         for feature in simple_stats_features:\n",
    "#             if(feature[0] == \"Įdėtas\"):\n",
    "#                 created_at =  feature[1]\n",
    "#             if(feature[0] == 'Redaguotas'):\n",
    "#                 updated_at = feature[1]\n",
    "        \n",
    "        dl_data = soup.find(\"dl\", class_=\"obj-details\")\n",
    "\n",
    "        features = []\n",
    "        for  dt_item in dl_data.find_all(\"dt\"):\n",
    "            feature_key = dt_item.text.strip()\n",
    "            \n",
    "            features.append([feature_key])\n",
    "            \n",
    "        for key, dt_item in enumerate(dl_data.find_all(\"dd\")):\n",
    "            feature_key = dt_item.text.strip()\n",
    "            features[key].append(feature_key)\n",
    "                \n",
    "        for feature in features:\n",
    "            if(feature[0] == \"Metai:\"):\n",
    "                years = re.sub( r\"([^0-9])\", r\" \\1\", feature[1]).split()[0]\n",
    "            if(feature[0] == \"Plotas:\"):\n",
    "                area =  re.sub('[^0-9\\,]', '', feature[1])\n",
    "            if(feature[0] == 'Kambarių sk.:'):\n",
    "                rooms = re.sub('[^0-9\\,]', '', feature[1])\n",
    "            if(feature[0] == 'Aukštas:'):\n",
    "                floor = re.sub('[^0-9\\,]', '', feature[1])\n",
    "            if(feature[0] == 'Pastato tipas:'):\n",
    "                building_type = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', feature[1])\n",
    "            if(feature[0] == 'Šildymas:'):\n",
    "                heating_type = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', feature[1])\n",
    "            if(feature[0] == 'Įrengimas:'):\n",
    "                installation = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', feature[1])\n",
    "               \n",
    "        soup = get_soup(get_ajax_url(get_id_from_url(url)))\n",
    "        \n",
    "        for item in soup.find_all(\"div\", \"statistic-info-row\"):\n",
    "            if(item.find(\"div\", \"icon-heating-gray\")):\n",
    "                heat = re.sub('[^0-9\\,]', '', item.find('span', 'cell-data cell-data-inline-block').contents[0])\n",
    "        \n",
    "        \n",
    "        return [city, years, area, rooms, heat, floor, building_type, heating_type, installation, price]\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_url(url):\n",
    "    return url.rsplit('-', 1)[1].replace('/', \"\")\n",
    "\n",
    "def get_ajax_url(item_id):\n",
    "    return \"https://www.aruodas.lt/ajax/getAdvertStatistics/?objTypeId=1&advertId=\"+item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"soup_aruodas_\"+time.strftime(\"%Y_%m_%d-%H%M\")+\".csv\"\n",
    " \n",
    "# prep file header\n",
    "write_row(['city', 'years', 'area', 'rooms', 'avg_heat_per_m', 'floor', 'building_type', 'heating_type', 'installation', 'price'], filename)\n",
    "    \n",
    "# get urls from page\n",
    "url = \"https://www.aruodas.lt/butai/puslapis/\"\n",
    "page = 1\n",
    "search = \"/?FHouseState=full\"\n",
    "    \n",
    "#total page number = 272\n",
    "while(page <= 272):\n",
    "    # get urls and cities from list page    \n",
    "    print(\"Page: \"+str(page))\n",
    "    cities_urls = get_ads_urls(url+str(page)+search)\n",
    "    page += 1\n",
    "    time.sleep(1)\n",
    "    for item in cities_urls:\n",
    "        data = get_data(item)\n",
    "        if(data):\n",
    "            print(\"Writing id: \"+ get_id_from_url(item['url']))\n",
    "            write_row(data, filename)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(\"Skipping id: \"+ get_id_from_url(item['url']))\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing\n",
    "url = \"https://www.aruodas.lt/butai/puslapis/\"\n",
    "page = 1\n",
    "search = \"/?FHouseState=full\"\n",
    "\n",
    "cities_urls = get_ads_urls(url+str(page)+search)\n",
    "\n",
    "data = get_data(cities_urls[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
