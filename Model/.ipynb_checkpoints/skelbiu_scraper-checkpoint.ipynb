{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row(rows, filename):\n",
    "    with open(filename, 'a', encoding='utf_8_sig') as toWrite:\n",
    "        writer = csv.writer(toWrite, lineterminator='\\n')\n",
    "        writer.writerow(rows)\n",
    "    toWrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
    "\n",
    "    # fetching the url, raising error if operation fails\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "        \n",
    "    return BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ads_urls(url, base):\n",
    "    soup = get_soup(url)\n",
    "    links = []\n",
    "    \n",
    "    for link in soup.find_all('a', 'adsImage js-cfuser-link'):\n",
    "        links.append(base+link.get('href'))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_url(url):\n",
    "    return url.rsplit('-', 1)[1].replace('.html', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    \n",
    "    \n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # logic to parse and return data about apartment\n",
    "    try:\n",
    "        price = re.sub('[^0-9\\,]', '', soup.find(\"p\", \"price\").text)\n",
    "        city = ''\n",
    "        years = ''\n",
    "        area = ''\n",
    "        rooms = ''\n",
    "        floor = ''\n",
    "        heating_type = ''\n",
    "        building_type = ''\n",
    "        installation = ''\n",
    "        \n",
    "        for details in soup.find_all('div', 'detail'):\n",
    "            title = details.find('div', 'title').text.strip()\n",
    "            if(title == 'Gyvenvietė:'):\n",
    "                city = details.find('div', 'value').get_text(strip=True, separator=',').strip().split(',')[0]\n",
    "            if(title == 'Kamb. sk.:'):\n",
    "                rooms = re.sub('[^0-9]', '', details.find('div', 'value').text)\n",
    "            if(title == 'Metai:'):\n",
    "                years = re.sub( r\"([^0-9])\", r\" \\1\", details.find('div', 'value').text).split()[0]\n",
    "            if(title == 'Plotas, m²:'):\n",
    "                area = re.sub('[^0-9\\,]', '', details.find('div', 'value').text)\n",
    "            if(title == 'Aukštas:'):\n",
    "                floor = re.sub('[^0-9\\,]', '', details.find('div', 'value').text)\n",
    "            if(title == 'Šildymas:'):\n",
    "                heating_type = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', details.find('div', 'value').text)\n",
    "            if(title == 'Tipas:'):\n",
    "                building_type = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', details.find('div', 'value').text)\n",
    "            if(title == 'Įrengimas:'):\n",
    "                installation = re.sub('[^a-zA-Z\\u00c0-\\u017e,]', '', details.find('div', 'value').text)\n",
    "        \n",
    "        return [city, years, area, rooms, floor, heating_type, building_type, installation, price]\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"soup_skelbiu_\"+time.strftime(\"%Y_%m_%d-%H%M\")+\".csv\"\n",
    "    \n",
    "# prep file header\n",
    "write_row(['city', 'years', 'area', 'rooms', 'floor', 'building_type', 'heating_type', 'installation', 'price'], filename)\n",
    "    \n",
    "# urls\n",
    "base = 'https://www.skelbiu.lt'\n",
    "base_url = \"https://www.skelbiu.lt/skelbimai/\"\n",
    "page = 1\n",
    "search = \"?autocompleted=1&keywords=&cost_min=&cost_max=&space_min=&space_max=&rooms_min=&rooms_max=&year_min=&year_max=&building=0&status=1&floor_min=&floor_max=&floor_type=0&import=2&searchAddress=&district=0&quarter=0&streets=0&ignorestreets=0&cities=0&distance=0&mainCity=0&search=1&category_id=41&type=1&user_type=0&ad_since_min=0&ad_since_max=0&visited_page=1&orderBy=-1&detailsSearch=1\"\n",
    "    \n",
    "    \n",
    "# collect only urls\n",
    "# scrap them (city, years, area, rooms, price, '', url)\n",
    "    \n",
    "while(page <= 312):\n",
    "    # get urls and cities from list page    \n",
    "    print(\"Page: \"+str(page))\n",
    "    urls = get_ads_urls(base_url+str(page)+search, base)\n",
    "    page += 1\n",
    "    time.sleep(.3)\n",
    "    for url in urls:\n",
    "        data = get_data(url)\n",
    "        if(data):\n",
    "            print(\"Writing id: \"+ get_id_from_url(url))\n",
    "            write_row(data, filename)\n",
    "            time.sleep(.555)\n",
    "        else:\n",
    "            print(\"Skipping id: \"+ get_id_from_url(url))\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing\n",
    "base = 'https://www.skelbiu.lt'\n",
    "base_url = \"https://www.skelbiu.lt/skelbimai/\"\n",
    "page = 1\n",
    "search = \"?autocompleted=1&keywords=&cost_min=&cost_max=&space_min=&space_max=&rooms_min=&rooms_max=&year_min=&year_max=&building=0&status=1&floor_min=&floor_max=&floor_type=0&import=2&searchAddress=&district=0&quarter=0&streets=0&ignorestreets=0&cities=0&distance=0&mainCity=0&search=1&category_id=41&type=1&user_type=0&ad_since_min=0&ad_since_max=0&visited_page=1&orderBy=-1&detailsSearch=1\"\n",
    "\n",
    "urls = get_ads_urls(base_url+str(page)+search, base)\n",
    "\n",
    "data = get_data(urls[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
